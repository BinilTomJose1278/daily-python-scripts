{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsiRsvZz2vTvvqcJRyW3H1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BinilTomJose1278/daily-python-scripts/blob/main/BreastCancerPrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9Ktkq_eXXLO"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries for machine learning tasks\n",
        "import numpy as np                # Library for numerical operations\n",
        "import pandas as pd               # Library for data manipulation and analysis\n",
        "from sklearn.model_selection import train_test_split  # Function to split data into training and testing sets\n",
        "from sklearn.preprocessing import StandardScaler      # Function to standardize features\n",
        "from sklearn.linear_model import LogisticRegression   # Logistic Regression algorithm\n",
        "from sklearn.ensemble import RandomForestClassifier   # Random Forest algorithm\n",
        "from sklearn.svm import SVC                           # Support Vector Machine algorithm\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report  # Metrics for model evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section imports essential libraries for our machine learning tasks. We use numpy for numerical operations, pandas for data manipulation and analysis, and various modules from scikit-learn for tasks such as data splitting, preprocessing, model training, and evaluation. These imports set the stage for our machine learning pipeline.\n"
      ],
      "metadata": {
        "id": "FQDuIfJwbQN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading a dataset using scikit-learn's built-in datasets\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = data.data    # Features\n",
        "y = data.target  # Target labels\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# train_test_split function splits the data into training (80%) and testing (20%) sets\n",
        "\n"
      ],
      "metadata": {
        "id": "MBEB5KdQXhc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start by loading a dataset using scikit-learn's built-in datasets. For this example, we use the Iris dataset, a classic dataset for classification tasks. The dataset is split into features (X) and target labels (y). We then use train_test_split to divide the dataset into training and testing sets, which is crucial for evaluating our models' performance on unseen data."
      ],
      "metadata": {
        "id": "gaFSPdgXbX70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardizing the features to have mean=0 and variance=1\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)  # Fit and transform the training data\n",
        "X_test = scaler.transform(X_test)        # Transform the testing data\n",
        "\n"
      ],
      "metadata": {
        "id": "nbsQdjZ9avS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing is a vital step in the machine learning workflow. Here, we standardize the features to have a mean of 0 and a variance of 1 using StandardScaler. Standardization improves the convergence of many learning algorithms by ensuring that all features contribute equally to the model's performance."
      ],
      "metadata": {
        "id": "0mFvqhQ9bqLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training a Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=10000)\n",
        "log_reg.fit(X_train, y_train)  # Fit the model to the training data\n",
        "\n",
        "# Making predictions with the trained model\n",
        "y_pred_log_reg = log_reg.predict(X_test)\n",
        "\n",
        "# Evaluating the model\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_log_reg))\n",
        "# accuracy_score function calculates the accuracy of the model\n",
        "print(confusion_matrix(y_test, y_pred_log_reg))\n",
        "# confusion_matrix function returns the confusion matrix\n",
        "print(classification_report(y_test, y_pred_log_reg))\n",
        "# classification_report function provides a detailed classification report\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFwQBVQfXoBV",
        "outputId": "bcb45491-087c-490f-ee31-de548e04b7a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.9736842105263158\n",
            "[[41  2]\n",
            " [ 1 70]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96        43\n",
            "           1       0.97      0.99      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.97      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression\n",
        "Explanation:\n",
        "Logistic Regression is a simple yet powerful classification algorithm. In this section, we train a Logistic Regression model using our training data. We then make predictions on the test set and evaluate the model's performance using accuracy, confusion matrix, and classification report. These metrics help us understand how well the model is performing."
      ],
      "metadata": {
        "id": "E0nsP0Bvb4qO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100)\n",
        "rf_clf.fit(X_train, y_train)  # Fit the model to the training data\n",
        "\n",
        "# Making predictions with the trained model\n",
        "y_pred_rf = rf_clf.predict(X_test)\n",
        "\n",
        "# Evaluating the model\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(confusion_matrix(y_test, y_pred_rf))\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ojGriltX6t-",
        "outputId": "e25ff576-ef39-4fb5-f857-6b0b2269fb92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 0.9649122807017544\n",
            "[[40  3]\n",
            " [ 1 70]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.93      0.95        43\n",
            "           1       0.96      0.99      0.97        71\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.97      0.96      0.96       114\n",
            "weighted avg       0.97      0.96      0.96       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest is an ensemble learning method that combines multiple decision trees to improve predictive performance. We train a Random Forest classifier and evaluate its performance using the same metrics as before. Ensemble methods like Random Forest are often more robust and accurate than individual models."
      ],
      "metadata": {
        "id": "-7JtHqaScE6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training a Support Vector Machine\n",
        "svm_clf = SVC()\n",
        "svm_clf.fit(X_train, y_train)  # Fit the model to the training data\n",
        "\n",
        "# Making predictions with the trained model\n",
        "y_pred_svm = svm_clf.predict(X_test)\n",
        "\n",
        "# Evaluating the model\n",
        "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
        "print(confusion_matrix(y_test, y_pred_svm))\n",
        "print(classification_report(y_test, y_pred_svm))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swX11Op4YDYg",
        "outputId": "bd99dab8-b928-4735-80b1-933d13fcca3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy: 0.9824561403508771\n",
            "[[41  2]\n",
            " [ 0 71]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.95      0.98        43\n",
            "           1       0.97      1.00      0.99        71\n",
            "\n",
            "    accuracy                           0.98       114\n",
            "   macro avg       0.99      0.98      0.98       114\n",
            "weighted avg       0.98      0.98      0.98       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machine (SVM) is a powerful algorithm for classification tasks. We train an SVM model on our training data and evaluate its performance. SVMs are effective in high-dimensional spaces and are versatile, as they can be used for both classification and regression tasks."
      ],
      "metadata": {
        "id": "ntM2in7xcYq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate the model using accuracy score, confusion matrix, and classification report\n",
        "def evaluate_model(y_true, y_pred):\n",
        "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
        "    print(\"Classification Report:\\n\", classification_report(y_true, y_pred))\n",
        "\n",
        "# Evaluating Logistic Regression\n",
        "evaluate_model(y_test, y_pred_log_reg)\n",
        "\n",
        "# Evaluating Random Forest\n",
        "evaluate_model(y_test, y_pred_rf)\n",
        "\n",
        "# Evaluating SVM\n",
        "evaluate_model(y_test, y_pred_svm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcJ1P5F0YMEM",
        "outputId": "d508babd-427a-49e8-efae-402b7e20d37f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9736842105263158\n",
            "Confusion Matrix:\n",
            " [[41  2]\n",
            " [ 1 70]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96        43\n",
            "           1       0.97      0.99      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.97      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n",
            "Accuracy: 0.9649122807017544\n",
            "Confusion Matrix:\n",
            " [[40  3]\n",
            " [ 1 70]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.93      0.95        43\n",
            "           1       0.96      0.99      0.97        71\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.97      0.96      0.96       114\n",
            "weighted avg       0.97      0.96      0.96       114\n",
            "\n",
            "Accuracy: 0.9824561403508771\n",
            "Confusion Matrix:\n",
            " [[41  2]\n",
            " [ 0 71]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.95      0.98        43\n",
            "           1       0.97      1.00      0.99        71\n",
            "\n",
            "    accuracy                           0.98       114\n",
            "   macro avg       0.99      0.98      0.98       114\n",
            "weighted avg       0.98      0.98      0.98       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model evaluation is crucial to understand how well our model generalizes to unseen data. We define a function to evaluate the model using accuracy, confusion matrix, and classification report. We then use this function to evaluate the Logistic Regression, Random Forest, and SVM models, providing a comprehensive view of their performance."
      ],
      "metadata": {
        "id": "dUow5TlXcgQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Cross-validation for Logistic Regression\n",
        "cv_scores = cross_val_score(log_reg, X, y, cv=5)\n",
        "# cross_val_score function performs cross-validation and returns scores for each fold\n",
        "print(\"Cross-validation scores (Logistic Regression):\", cv_scores)\n",
        "print(\"Mean cross-validation score (Logistic Regression):\", np.mean(cv_scores))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8lZNNAGYPju",
        "outputId": "322905bf-62bd-45c3-f777-2e9f29eecbff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation scores (Logistic Regression): [0.93859649 0.94736842 0.98245614 0.92982456 0.95575221]\n",
            "Mean cross-validation score (Logistic Regression): 0.9507995652848935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-validation is a technique to assess how well our model generalizes to an independent dataset. It involves splitting the dataset into multiple folds and training the model on each fold. We perform cross-validation for the Logistic Regression model and report the scores, providing insights into the model's stability and reliability.\n",
        "\n"
      ],
      "metadata": {
        "id": "TSDqDL4pdHeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Hyperparameter tuning for Random Forest\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],  # Number of trees in the forest\n",
        "    'max_depth': [None, 10, 20, 30]  # Maximum depth of the tree\n",
        "}\n",
        "grid_search = GridSearchCV(estimator=rf_clf, param_grid=param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)  # Fit the model to the training data with cross-validation\n",
        "\n",
        "print(\"Best parameters found:\", grid_search.best_params_)\n",
        "print(\"Best cross-validation score:\", grid_search.best_score_)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CbOnyzsYZom",
        "outputId": "f52a9bd7-9594-4be0-9be1-d49d1c870bde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found: {'max_depth': None, 'n_estimators': 50}\n",
            "Best cross-validation score: 0.9648351648351647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tuning is the process of optimizing a model's hyperparameters to improve performance. We use GridSearchCV to perform an exhaustive search over a specified parameter grid for the Random Forest classifier. This section demonstrates how to find the best combination of hyperparameters to enhance the model's accuracy."
      ],
      "metadata": {
        "id": "sbeglO2Cd9-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Saving the Logistic Regression model\n",
        "joblib.dump(log_reg, 'logistic_regression_model.pkl')\n",
        "\n",
        "# Loading the saved model\n",
        "loaded_model = joblib.load('logistic_regression_model.pkl')\n",
        "\n"
      ],
      "metadata": {
        "id": "iaXvjfR1YiKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving a trained model allows us to reuse it without retraining. We use joblib to save the Logistic Regression model to a file. This is particularly useful for deploying models to production environments. We also demonstrate how to load the saved model and use it for predictions."
      ],
      "metadata": {
        "id": "ZhMGonSzeD7m"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aC7cdmeGXlxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting with the loaded model\n",
        "y_pred_loaded = loaded_model.predict(X_test)\n",
        "print(\"Loaded Model Accuracy:\", accuracy_score(y_test, y_pred_loaded))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZGE7AWmYl1Z",
        "outputId": "3e196a88-9594-41db-9a87-2d7db2b663aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded Model Accuracy: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After loading a previously saved model, we make predictions on the test set to ensure that the model's performance is consistent. This step is crucial for validating that the saved model can be effectively reused for making predictions on new data."
      ],
      "metadata": {
        "id": "nE5I7jVseF5H"
      }
    }
  ]
}